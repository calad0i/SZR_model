{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQL constants\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE_EVERY = 10\n",
    "LEARNING_RATE = 5e-4\n",
    "NUM_EPISODES = 20\n",
    "\n",
    "# Environment constants\n",
    "ENV_SIZE = 32\n",
    "MAX_ENV_STEPS = 50\n",
    "POP_DENSITY = 0.2\n",
    "ZOMBIE_FRACTION = 0.3\n",
    "VISIBILITY = 7 # square's half side length\n",
    "STATE_ROOT_SIZE = 2 * VISIBILITY + 1\n",
    "STATE_SIZE = STATE_ROOT_SIZE**2\n",
    "ACTION_SIZE = 5 # 4 directions + do nothing\n",
    "\n",
    "DEATH_REWARD = -100\n",
    "KILL_REWARD = 50\n",
    "REST_REWARD = 0\n",
    "MOVE_REWARD = -5\n",
    "\n",
    "EMPTY_CELL = 0\n",
    "HUMAN_CELL = 1\n",
    "AGENT_CELL = 2\n",
    "ZOMBIE_CELL = 3\n",
    "\n",
    "# Printing and visualization constants\n",
    "SHOW_ENV = True\n",
    "SHOW_ENV_EVERY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "        \n",
    "    def __init__(self):\n",
    "        print(\"Initializing environment\")\n",
    "    \n",
    "    def reset(self):\n",
    "        ## generate grid\n",
    "        self.grid = np.zeros((ENV_SIZE, ENV_SIZE), dtype=np.uint8)\n",
    "        \n",
    "        ## place people and zombies\n",
    "        rands = [random.uniform(0, 1) for i in range(ENV_SIZE**2)]\n",
    "        r = random.randint(0, ENV_SIZE**2-1)\n",
    "        self.humans_pos = []\n",
    "        self.zombies_pos = []\n",
    "        for y in range(ENV_SIZE):\n",
    "            for x in range(ENV_SIZE):\n",
    "                if rands[y * ENV_SIZE + x] < POP_DENSITY:\n",
    "                    if rands[y * ENV_SIZE + x] < POP_DENSITY * ZOMBIE_FRACTION:\n",
    "                        self.grid[y, x] = ZOMBIE_CELL\n",
    "                        self.zombies_pos.append((y, x))\n",
    "                    else:\n",
    "                        self.grid[y, x] = HUMAN_CELL\n",
    "                        self.humans_pos.append((y, x))\n",
    "                    \n",
    "        # place agent\n",
    "        self.agent_x = random.randint(0, ENV_SIZE-1) # (second bound is included)\n",
    "        self.agent_y = random.randint(0, ENV_SIZE-1)\n",
    "        while self.grid[self.agent_y, self.agent_x] != EMPTY_CELL:\n",
    "            self.agent_x = random.randint(0, ENV_SIZE-1)\n",
    "            self.agent_y = random.randint(0, ENV_SIZE-1)\n",
    "        self.grid[self.agent_y, self.agent_x] = AGENT_CELL\n",
    "        \n",
    "        self.episode_step = 0\n",
    "        state = self.scan_env(self.agent_x, self.agent_y)\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        \n",
    "        self.perform_action(action)\n",
    "        self.zombies_pos = self.move_zombies()\n",
    "        ## TODO - Later on: make other agents move as well\n",
    "        \n",
    "        dead_humans = set()\n",
    "        dead_zombies = set()\n",
    "        \n",
    "        nb_allies, enemies = self.scan_vicinity(self.agent_x, self.agent_y)\n",
    "        \n",
    "        # Handle agent's interactions\n",
    "        dead = False\n",
    "        reward = REST_REWARD if action == 0 else MOVE_REWARD\n",
    "        if len(enemies) > 0: # Contact\n",
    "            for enemy in enemies: #Might want to aggregate this by simply modifying the 'b' value instead (b = 1 + nb_enemies)\n",
    "                r = np.random.uniform(0, 1)\n",
    "                b = 1\n",
    "                k = 1 + nb_allies # << S # maybe add some multiplicative factor to put more importance on having allies\n",
    "                if r >= b/(b+k): # kill\n",
    "                    reward += KILL_REWARD\n",
    "                    self.grid[enemy] = EMPTY_CELL\n",
    "                    dead_zombies.add(enemy)\n",
    "                else: # death\n",
    "                    reward += DEATH_REWARD\n",
    "                    dead = True\n",
    "                    ## Probably don't need to update state as it won't be used in replay memory anyway\n",
    "                    break\n",
    "        done = self.episode_step >= MAX_ENV_STEPS or dead\n",
    "        \n",
    "        # Handle all other interactions\n",
    "        for (y, x) in self.humans_pos:\n",
    "            nb_allies, enemies = self.scan_vicinity(x, y)\n",
    "            if len(enemies) > 0:\n",
    "                for enemy in enemies:\n",
    "                    r = np.random.uniform(0, 1)\n",
    "                    b = 1\n",
    "                    k = 1 + nb_allies # << S\n",
    "                    if r >= b/(b+k): # kill\n",
    "                        self.grid[enemy] = EMPTY_CELL\n",
    "                        dead_zombies.add(enemy)\n",
    "                    else: # death\n",
    "                        self.grid[y, x] = ZOMBIE_CELL\n",
    "                        self.zombies_pos.append((y, x))\n",
    "                        dead_humans.add((y, x))\n",
    "        \n",
    "        # Clean the dead\n",
    "        for dead_human in dead_humans:\n",
    "            self.humans_pos.remove(dead_human)\n",
    "        for dead_zombie in dead_zombies:\n",
    "            self.zombies_pos.remove(dead_zombie)\n",
    "            \n",
    "        state = self.scan_env(self.agent_x, self.agent_y)\n",
    "        \n",
    "        return state, reward, done, None\n",
    "    \n",
    "    def scan_vicinity(self, x, y): #Scan the vicinity to record allies and enemies\n",
    "        enemies = []\n",
    "        nb_allies = 0\n",
    "        for j in [(y-1)%ENV_SIZE, y, (y+1)%ENV_SIZE]:\n",
    "            for i in [(x-1)%ENV_SIZE, x, (x+1)%ENV_SIZE]:\n",
    "                if not (j == y and i == x):\n",
    "                    if self.grid[j, i] == HUMAN_CELL:\n",
    "                        nb_allies += 1\n",
    "                    elif self.grid[j, i] == ZOMBIE_CELL and (j == y or i == x):\n",
    "                        enemies.append((j, i))\n",
    "        return nb_allies, enemies\n",
    "    \n",
    "    def scan_env(self, x, y): #Scan environment to return the state\n",
    "        state = np.zeros((STATE_SIZE,))\n",
    "        k = 0\n",
    "        for j in range(y - VISIBILITY, y + VISIBILITY+1):\n",
    "            for i in range(x - VISIBILITY, x + VISIBILITY+1):\n",
    "                state[k] = self.grid[j%ENV_SIZE, i%ENV_SIZE]\n",
    "                k += 1\n",
    "        state = state.reshape((1, STATE_ROOT_SIZE, STATE_ROOT_SIZE))\n",
    "        state = np.ascontiguousarray(state, dtype=np.float32)\n",
    "        state = torch.from_numpy(state)\n",
    "        state = state.unsqueeze(0)\n",
    "        return state\n",
    "    \n",
    "    def perform_action(self, action):\n",
    "        x = self.agent_x\n",
    "        y = self.agent_y\n",
    "        if action == 0:\n",
    "            return\n",
    "        if action == 1: #move right\n",
    "            self.agent_x = (self.agent_x + 1) % ENV_SIZE\n",
    "        elif action == 2: #move left\n",
    "            self.agent_x = (self.agent_x - 1) % ENV_SIZE\n",
    "        elif action == 3: #move top\n",
    "            self.agent_y = (self.agent_y + 1) % ENV_SIZE\n",
    "        else: #move down\n",
    "            self.agent_y = (self.agent_y - 1) % ENV_SIZE\n",
    "        # Might want to implement collision avoidance, but this might cause problems if 2 agents want to move\n",
    "        # together in the same direction.\n",
    "        if self.grid[self.agent_y, self.agent_x] != EMPTY_CELL: # If cell not empty\n",
    "            self.agent_x = x\n",
    "            self.agetn_y = y\n",
    "        else:\n",
    "            self.grid[y, x] = EMPTY_CELL\n",
    "            self.grid[self.agent_y, self.agent_x] = AGENT_CELL\n",
    "    \n",
    "    def move_zombies(self):\n",
    "        new_pos = []\n",
    "        for (y, x) in self.zombies_pos:\n",
    "            action = random.randint(0, 4)\n",
    "            if action == 0:\n",
    "                new_pos.append((y, x))\n",
    "            else:\n",
    "                new_x = x\n",
    "                new_y = y\n",
    "                if action == 1: #move right\n",
    "                    new_x = (x + 1) % ENV_SIZE\n",
    "                elif action == 2: #move left\n",
    "                    new_x = (x - 1) % ENV_SIZE\n",
    "                elif action == 3: #move top\n",
    "                    new_y = (y + 1) % ENV_SIZE\n",
    "                else: #move down\n",
    "                    new_y = (y - 1) % ENV_SIZE\n",
    "                if self.grid[new_y, new_x] != EMPTY_CELL:\n",
    "                    new_x = x\n",
    "                    new_y = y\n",
    "                else:\n",
    "                    self.grid[y, x] = EMPTY_CELL\n",
    "                    self.grid[new_y, new_x] = ZOMBIE_CELL\n",
    "                new_pos.append((new_y, new_x))\n",
    "        return new_pos\n",
    "        \n",
    "    def render(self):\n",
    "        img = self.get_image()\n",
    "        img = img.resize((300, 300))\n",
    "        cv2.imshow(\"image\", np.array(img))\n",
    "        cv2.waitKey(1)\n",
    "    \n",
    "    def get_image(self):\n",
    "        env = self.grid * 80\n",
    "        #env = env[:, None]\n",
    "        img = Image.fromarray(env, 'L')  # Black and white mode\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, w, h, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(0)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(w))\n",
    "        convh = conv2d_size_out(conv2d_size_out(h))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "        # DQNs\n",
    "        self.policy_net = DQN(STATE_ROOT_SIZE, STATE_ROOT_SIZE, ACTION_SIZE).to(device)\n",
    "        self.target_net = DQN(STATE_ROOT_SIZE, STATE_ROOT_SIZE, ACTION_SIZE).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = ReplayMemory(BUFFER_SIZE)\n",
    "        \n",
    "        self.time_step = 0\n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = next_state_values * GAMMA + reward_batch\n",
    "        \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1) # Gradient clipping?\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.time_step / EPS_DECAY)\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(ACTION_SIZE)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def train(self):\n",
    "        for episode in tqdm(range(NUM_EPISODES)):\n",
    "            state = self.env.reset() # get initial state\n",
    "            #state = torch.from_numpy(state)\n",
    "            for t in count(): # The environment is responsible for returning done=True after some time steps\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action.item())\n",
    "                #next_state = torch.from_numpy(next_state)\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                \n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "                \n",
    "                self.optimize_model()\n",
    "                if done:\n",
    "                    # TODO: Plot some statistics etc...\n",
    "                    break\n",
    "            if episode % TARGET_UPDATE_EVERY == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "                \n",
    "            if SHOW_ENV and episode % SHOW_ENV_EVERY == 0:\n",
    "                self.env.render()\n",
    "        print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:04<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = Environment()\n",
    "dqlAgent = DQLAgent(env)\n",
    "dqlAgent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
