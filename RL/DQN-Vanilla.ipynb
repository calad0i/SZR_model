{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import math\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQL constants\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.95\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE_EVERY = 10\n",
    "LEARNING_RATE = 5e-4\n",
    "NUM_EPISODES = 20\n",
    "\n",
    "# Environment constants\n",
    "ENV_SIZE = 32\n",
    "MAX_ENV_STEPS = 50\n",
    "POP_DENSITY = 0.2\n",
    "ZOMBIE_FRACTION = 0.3\n",
    "VISIBILITY = 4 # square's half side length\n",
    "STATE_SIZE = (2 * VISIBILITY + 1)**2\n",
    "ACTION_SIZE = 5 # 4 directions + do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code the environment\n",
    "class Environment():\n",
    "    DEATH_REWARD = -100\n",
    "    KILL_REWARD = 50\n",
    "    REST_REWARD = 0\n",
    "    MOVE_REWARD = -5\n",
    "    \n",
    "    EMPTY_CELL = 0\n",
    "    HUMAN_CELL = 1\n",
    "    AGENT_CELL = 2\n",
    "    ZOMBIE_CELL = 3\n",
    "    \n",
    "    def __init__(self, state_size):\n",
    "        self.state_size = state_size\n",
    "    \n",
    "    def reset(self):\n",
    "        ## generate grid\n",
    "        self.grid = np.zeros((ENV_SIZE, ENV_SIZE))\n",
    "        \n",
    "        ## place people and zombies\n",
    "        rands = [random.uniform(0, 1) for i in range(ENV_SIZE**2)]\n",
    "        r = random.randint(0, ENV_SIZE**2-1)\n",
    "        for y in range(ENV_SIZE):\n",
    "            for x in range(ENV_SIZE):\n",
    "                if rands[y * ENV_SIZE + x] < POP_DENSITY:\n",
    "                    if rands[y * ENV_SIZE + x] < POP_DENSITY * ZOMBIE_FRACTION:\n",
    "                        grid[y, x] = ZOMBIE_CELL\n",
    "                    else:\n",
    "                        grid[y, x] = HUMAN_CELL\n",
    "                    \n",
    "        # place agent\n",
    "        self.agent_x = random.randint(0, ENV_SIZE-1) # (second bound is included)\n",
    "        self.agent_y = random.randint(0, ENV_SIZE-1)\n",
    "        while grid[self.agent_y, self.agent_x] != EMPTY_CELL:\n",
    "            self.agent_x = random.randint(0, ENV_SIZE-1)\n",
    "            self.agent_y = random.randint(0, ENV_SIZE-1)\n",
    "        grid[self.agent_y, self.agent_x] = AGENT_CELL\n",
    "        \n",
    "        self.episode_step = 0\n",
    "        state, _, _ = scan_env(self.agent_pos)\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.episode_step += 1\n",
    "        self.perform_action(action)\n",
    "        \n",
    "        ## TODO - Later on: make other agents move as well\n",
    "        \n",
    "        self.move_zombies()\n",
    "        \n",
    "        state, nb_allies, enemies = scan_env(self.agent_pos)\n",
    "        \n",
    "        dead = False\n",
    "        reward = REST_REWARD if action == 0 else MOVE_REWARD\n",
    "        \n",
    "        # TODO: do this same thing but for all other interactions\n",
    "        if len(enemies) > 0: # Contact\n",
    "            for enemy in range(enemies): #Might want to aggregate this by simply modifying the 'b' value instead (b = 1 + nb_enemies)\n",
    "                r = np.random.uniform(0, 1)\n",
    "                b = 1\n",
    "                k = 1 + nb_allies # << S # maybe add some multiplicative factor to put more importance on having allies\n",
    "                if r >= b/(b+k): # kill\n",
    "                    reward += KILL_REWARD\n",
    "                    self.grid[enemy] = EMPTY_CELL # TODO make sure this works correctly (do not invert y and x coos..)\n",
    "                else: # death\n",
    "                    reward += DEATH_REWARD\n",
    "                    dead = True\n",
    "                    ## Probably don't need to update state as it won't be used in replay memory anyway\n",
    "                    break\n",
    "        \n",
    "        done = self.episode_step >= MAX_ENV_STEPS or dead\n",
    "        \n",
    "        return state, reward, done, None\n",
    "    \n",
    "    def scan_env(self, agent_pos): #Scan the vicinity to record allies and enemies (this is our new state)\n",
    "        # TODO\n",
    "        state = np.array([])\n",
    "        nb_allies = 0\n",
    "        nb_enemies = 0\n",
    "        return state, nb_allies, nb_enemies\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def perform_action(self, action):\n",
    "        x = self.agent_x\n",
    "        y = self.agent_y\n",
    "        if action == 1: #move right\n",
    "            self.agent_x = (self.agent_x + 1) % ENV_SIZE\n",
    "        elif action == 2: #move left\n",
    "            self.agent_x = (self.agent_x - 1) % ENV_SIZE\n",
    "        elif action == 3: #move top\n",
    "            self.agent_y = (self.agent_y + 1) % ENV_SIZE\n",
    "        elif action == 4: #move down\n",
    "            self.agent_y = (self.agent_y - 1) % ENV_SIZE\n",
    "        # Might want to implement collision avoidance, but this might cause problems if 2 agents want to move\n",
    "        # together in the same direction.\n",
    "        if grid[self.agent_y, self.agent_x] != 0: # If cell not empty\n",
    "            self.agent_x = x\n",
    "            self.agetn_y = y\n",
    "        else:\n",
    "            grid[y, x] = EMPTY_CELL\n",
    "            grid[self.agent_y, self.agent_x] = HUMAN_CELL\n",
    "    \n",
    "    def move_zombies(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.seed = torch.manual_seed(0)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.head = nn.Linear(32, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # return self.head(x)\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent():\n",
    "    def __init__(self, state_size, action_size, env):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.env = env\n",
    "        \n",
    "        # DQNs\n",
    "        self.policy_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net = DQN(state_size, action_size).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        self.memory = ReplayMemory(BUFFER_SIZE)\n",
    "        \n",
    "        self.time_step = 0\n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = next_state_values * GAMMA + reward_batch\n",
    "        \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1) # Gradient clipping?\n",
    "        optimizer.step()\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.time_step / EPS_DECAY)\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.action_size)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def train(self):\n",
    "        for episode in range(NUM_EPISODES):\n",
    "            state = self.env.reset() # get initial state\n",
    "            for t in count(): # The environment is responsible for returning done=True after some time steps\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action.item())\n",
    "                reward = torch.tensor([reward], device=device)\n",
    "                \n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "                \n",
    "                self.optimize_model()\n",
    "                if done:\n",
    "                    # TODO: Plot some statistics etc...\n",
    "                    break\n",
    "            if episode % TARGET_UPDATE_EVERY == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        print(\"Training finished\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(STATE_SIZE)\n",
    "dqlAgent = DQLAgent(STATE_SIZE, ACTION_SIZE, env)\n",
    "#dqlAgent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
